# Second Scenario - Server

## General Description

This scenario implements a reinforcement learning agent (Q-learning) for dynamic optimisation of system parameters on a Linux server running nginx.
The agent monitors server metrics in real time (CPU, IO, network, RPS, etc.), applies kernel/network tuning actions, and learns to maximise throughput and stability under synthetic HTTP load.

---

## Folder Structure

```
Second Scenario - Server/
├── agent_server.py               # RL agent (Q-learning): actions, Q-table, monitoring, reward
├── train_server_agent.py         # RL training loop, load management, Q-table/config saving
├── heuristic_agent_server.py     # Heuristic agent (simple rules based on metrics)
├── random_agent_server.py        # Random agent (random actions)
├── no_op_policy_server.py        # No-op agent (baseline, does nothing)
├── load_generator.py             # HTTP load generator (wrk), metrics parsing
├── q_table_server.npy            # (Generated) Saved Q-table
├── best_configs.json             # (Generated) Best configurations found
├── rewards/                      # (Generated) Rewards per episode for each strategy
├── plots/                        # (Generated) Reward plots
├── compare_strategies_server.py  # Script to compare strategies (plots)
├── compare_strategies_server.png # (Generated) Reward plot of each strategy 
└── README.md
```

---

## Dependencies and Installation

### System Requirements

- Linux (root access required for system tuning)
- nginx (web server)
- wrk (HTTP benchmarking tool)
- Python 3.8+

### Python Dependencies

From the root folder:

```bash
pip install -r requirements.txt
```

### System Tools Installation (example for Ubuntu)

```bash
sudo apt update
sudo apt install nginx wrk
```

---

## Usage

### 1. **Start nginx**

Make sure nginx is running and serving a static file (e.g. `/server.html`):

```bash
sudo systemctl start nginx
```

### 2. **Run an agent**

- **RL agent (Q-learning):**

```bash
python3 train_server_agent.py
```

- **Heuristic agent:**

```bash
python3 heuristic_agent_server.py
```

- **Random agent:**

```bash
python3 random_agent_server.py
```

- **No-op agent:**

```bash
python3 no_op_policy_server.py
```

### 3. **Compare strategies**

To plot and compare rewards for different strategies:

```bash
python3 compare_strategies_server.py
```

---


## Load generation & metrics collection

HTTP load is generated by `wrk` (see [`load_generator.py`](load_generator.py)), which bombards nginx via HTTP.

After each action, the agent runs a short wrk benchmark (duration, threads, connections are configurable).

Extracted metrics:

  - **RPS** (requests per second)
  - **latency** (average latency)
  - **p99** (99th percentile latency, if available)
  - **cpu_usage** and **mem_usage** for nginx (via psutil)

Each agent/policy collects these metrics after every action to compute the reward.

---

## Reward calculation

The main reward is based on RPS (the higher, the better).

Penalties are applied if:

- Latency increases (proportional subtraction)
- nginx CPU or memory usage exceeds certain thresholds
- RPS drops sharply compared to the previous step (stability penalty)

See the `compute_reward` method in [`agent_server.py`](agent_server.py) for details.

---

## Main files

- **agent_server.py**: Defines the RL agent, actions, Q-table, reward calculation.

- **train_server_agent.py**: RL training loop, load management, results saving.

- **heuristic_agent_server.py**: Agent using simple rules based on metrics.

- **random_agent_server.py**: Agent choosing actions randomly.

- **no_op_policy_server.py**: Agent that does nothing.

- **load_generator.py**: Runs wrk, extracts performance metrics.

- **compare_strategies_server.py**: Generates comparison plots between strategies.

---

## Expected Results

- **Q-table**: Gradually filled, guides the agent to the best actions for each server state.
- **Best configs**: Top configurations (kernel/network parameters) found during training, exportable as JSON.
- **Plots**: Moving average of reward per episode, visualising learning progress.
- **Console logs**: Actions applied, metrics, and reward details for each episode.

---

## Tips

- **Run scripts as root** to allow all system actions.
- **Adjust bins and episode count** in `agent_server.py` and `train_server_agent.py` according to your server’s power and desired training time.
- **Monitor nginx and wrk** to ensure the server is running and accessible during training.
- **Interrupt safely**: The script saves the Q-table and resets system parameters on keyboard interrupt.
